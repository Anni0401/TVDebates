{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katharina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\inspect.py:746: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\Katharina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytubefix\n",
    "import whisperx\n",
    "import gc \n",
    "device = \"cpu\" \n",
    "\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"int8\" # change to \"int8\" if low on GPU mem (may reduce accuracy)torch.serialization.default_load_weights_only = False\n",
    "\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\",device, compute_type=compute_type)\n",
    "\n",
    "#model = whisperx.load_model(\"large-v2\", \"cpu\")\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='audio.mp3'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "link = \"https://www.youtube.com/watch?v=3itfVgCN0sU\"\n",
    "yt = pytubefix.YouTube(link)\n",
    "\n",
    "# Herunterladen des Audiostreams mit der höchsten verfügbaren Qualität\n",
    "audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "mp3_file = \"audio.mp3\"\n",
    "\n",
    "audio_stream.download(filename=\"temp_audio.m4a\")\n",
    "\n",
    "# Umwandlung von M4A zu MP3 (benötigt ffmpeg oder pydub)\n",
    "from pydub import AudioSegment\n",
    "\n",
    "audio = AudioSegment.from_file(\"temp_audio.m4a\", format=\"m4a\")\n",
    "audio.export(mp3_file, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: de (1.00) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio = whisperx.load_audio(mp3_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_access_token, device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegments\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m segment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Ende: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "for segment in result[\"segments\"]:\n",
    "    for word in segment[\"words\"]:\n",
    "        print(f\"{word['text']} - Start: {word['start']}, Ende: {word['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the transcribed text\n",
    "transcribed_text = result[\"text\"]\n",
    "\n",
    "with open(\"transcription_\" + name + \".txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge speaker diarization and transcription\n",
    "diarization_segments = []\n",
    "min_duration = 1.0  #Minimum duration for a speach in seconds\n",
    "tolerance = 0.2  # Allows small deviations for the first words\n",
    "\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_duration = segment.end - segment.start\n",
    "    if segment_duration >= min_duration:\n",
    "        diarization_segments.append((segment.start, segment.end, speaker))\n",
    "\n",
    "active_speaker = None\n",
    "combined_output = []\n",
    "merged_start = None\n",
    "merged_end = None\n",
    "merged_speaker = None\n",
    "merged_text = []\n",
    "\n",
    "# Check which speaker is assigned to each word in the transcription\n",
    "for segment in result[\"segments\"]:\n",
    "    for word in segment[\"words\"]:\n",
    "        current_time = word[\"start\"]\n",
    "\n",
    "        # Find the matching speaker segment based on the timestamps\n",
    "        closest_speaker = None\n",
    "        for start, end, speaker in diarization_segments:\n",
    "            # Check whether the word is within a segment or just before it (tolerance)\n",
    "            if start - tolerance <= current_time <= end:\n",
    "                closest_speaker = speaker\n",
    "                break \n",
    "\n",
    "        # When a speaker has been found\n",
    "        if closest_speaker is not None:\n",
    "            # If the speaker changes or no speaker is set yet\n",
    "            if active_speaker != closest_speaker:\n",
    "                # If there has been a change of speaker, write the previous text\n",
    "                if merged_speaker:\n",
    "                    speaker_name = speaker_names.get(merged_speaker, \"Unknown Speaker\")\n",
    "                    combined_output.append(f\"{speaker_name}: {' '.join(merged_text)}\")\n",
    "                \n",
    "                # Set variables for the new speaker\n",
    "                merged_speaker = closest_speaker\n",
    "                merged_start = word[\"start\"]\n",
    "                merged_end = word[\"end\"]\n",
    "                merged_text = [word[\"text\"]]\n",
    "                active_speaker = closest_speaker  # Update active speaker\n",
    "            else:\n",
    "                # If the speaker remains the same, add the word\n",
    "                merged_end = word[\"end\"]\n",
    "                merged_text.append(word[\"text\"])\n",
    "        else:\n",
    "            # If no speaker can be assigned, append the word to the last speaker\n",
    "            if merged_speaker:\n",
    "                merged_text.append(word[\"text\"])\n",
    "\n",
    "# Add last merged speaker, if present\n",
    "if merged_speaker:\n",
    "    speaker_name = speaker_names.get(merged_speaker, \"Unknown Speaker\")\n",
    "    combined_output.append(f\"{speaker_name}: {' '.join(merged_text)}\")\n",
    "\n",
    "# Save output\n",
    "output_file = \"combined_output\" + name + \".txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in combined_output:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
